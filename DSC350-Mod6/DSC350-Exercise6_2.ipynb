{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: Andres Melendez\n",
    "Date: 2024-10-01\n",
    "Description: This script filters earthquake data for events in Japan with a magnitude of 4.9 or greater using \n",
    "the 'mb' magnitude type, ensuring that the DataFrame includes the necessary columns.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def filter_earthquakes_by_magnitude(df, location, min_magnitude, magnitude_type):\n",
    "    \"\"\"\n",
    "    Filters earthquakes by a specific location keyword, magnitude type, and minimum magnitude.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing earthquake data.\n",
    "        location (str): The location keyword to filter (e.g., 'Japan').\n",
    "        min_magnitude (float): The minimum magnitude.\n",
    "        magnitude_type (str): The magnitude type to filter by (e.g., 'mb').\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Filtered DataFrame containing earthquakes that meet the criteria.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ensure the DataFrame contains necessary columns\n",
    "        required_columns = ['parsed_place', 'mag', 'magType']\n",
    "        if not all(col in df.columns for col in required_columns):\n",
    "            raise ValueError(\"Required columns are missing from the DataFrame\")\n",
    "\n",
    "        # Convert 'mag' column to numeric, handling any potential errors with invalid values\n",
    "        df['mag'] = pd.to_numeric(df['mag'], errors='coerce')\n",
    "\n",
    "        # Filter based on location, magnitude type, and magnitude value\n",
    "        filtered_df = df[(df['parsed_place'] == location) & \n",
    "                         (df['magType'] == magnitude_type) & \n",
    "                         (df['mag'] >= min_magnitude)]\n",
    "        return filtered_df\n",
    "\n",
    "    except ValueError as e:\n",
    "        # Capture and print errors related to missing columns\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "    finally:\n",
    "        # Optional cleanup actions, if necessary\n",
    "        print(\"Filtering process completed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering process completed.\n",
      "      mag magType           time                         place  tsunami parsed_place\n",
      "1563  4.9      mb  1538977532250  293km ESE of Iwo Jima, Japan        0        Japan\n",
      "2576  5.4      mb  1538697528010    37km E of Tomakomai, Japan        0        Japan\n",
      "3072  4.9      mb  1538579732490     15km ENE of Hasaki, Japan        0        Japan\n",
      "3632  4.9      mb  1538450871260    53km ESE of Hitachi, Japan        0        Japan\n"
     ]
    }
   ],
   "source": [
    "# Load the earthquake data from the uploaded CSV file\n",
    "file_path = 'data/Mod6/earthquakes.csv'\n",
    "earthquake_data = pd.read_csv(file_path)\n",
    "\n",
    "# Call the function to filter earthquakes in Japan with mb magnitude >= 4.9\n",
    "japan_earthquakes = filter_earthquakes_by_magnitude(earthquake_data, 'Japan', 4.9, 'mb')\n",
    "\n",
    "# Display filtered DataFrame only if it is not empty or None\n",
    "if japan_earthquakes is not None and not japan_earthquakes.empty:\n",
    "    print(japan_earthquakes.to_string())  # Print the filtered DataFrame\n",
    "else:\n",
    "    print(\"No data to display.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binned_mag\n",
      "(0, 1]    2207\n",
      "(1, 2]    3105\n",
      "(2, 3]     862\n",
      "(3, 4]     122\n",
      "(4, 5]       2\n",
      "(5, 6]       1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Filter and modify earthquake_data in one step, using .loc to avoid SettingWithCopyWarning\n",
    "earthquake_data.loc[earthquake_data['magType'] == 'ml', 'mag'] = pd.to_numeric(earthquake_data['mag'], errors='coerce')\n",
    "\n",
    "# Define the bin edges\n",
    "bins = pd.interval_range(start=0, end=int(earthquake_data.loc[earthquake_data['magType'] == 'ml', 'mag'].max()) + 1, freq=1)\n",
    "\n",
    "# Bin the data directly in earthquake_data\n",
    "earthquake_data.loc[earthquake_data['magType'] == 'ml', 'binned_mag'] = pd.cut(earthquake_data['mag'], bins=bins)\n",
    "\n",
    "# Count how many earthquakes fall into each bin\n",
    "binned_counts = earthquake_data.loc[earthquake_data['magType'] == 'ml', 'binned_mag'].value_counts().sort_index()\n",
    "\n",
    "# Display the binned counts\n",
    "print(binned_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ticker        date        high         low        open       close  \\\n",
      "0     FB  2018-01-02  181.580002  177.550003  177.679993  181.419998   \n",
      "1     FB  2018-01-03  184.779999  181.330002  181.880005  184.669998   \n",
      "2     FB  2018-01-04  186.210007  184.100006  184.899994  184.330002   \n",
      "3     FB  2018-01-05  186.899994  184.929993  185.589996  186.850006   \n",
      "4     FB  2018-01-08  188.899994  186.330002  187.199997  188.279999   \n",
      "\n",
      "       volume  \n",
      "0  18151900.0  \n",
      "1  16886600.0  \n",
      "2  13880900.0  \n",
      "3  13574500.0  \n",
      "4  17994700.0  \n"
     ]
    }
   ],
   "source": [
    "# Description: This script attempts to load the faang.csv file and displays the first few rows for data inspection.\n",
    "\n",
    "def load_faang_data(file_path):\n",
    "    \"\"\"\n",
    "    Function to load the FAANG data from a CSV file and display the first few rows.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Loaded FAANG data as a Pandas DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        faang_data = pd.read_csv(file_path)\n",
    "        print(faang_data.head())\n",
    "        return faang_data\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: The file was not found at the specified path.\")\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(\"Error: The file is empty.\")\n",
    "    except pd.errors.ParserError:\n",
    "        print(\"Error: There was an error parsing the file.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# Sample usage\n",
    "file_path = 'data/Mod6/faang.csv'\n",
    "faang_data = load_faang_data(file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ticker       date       open       high        low      close        volume\n",
      "0   AAPL 2018-01-31  43.505357  45.025002  41.174999  43.501309  2.638718e+09\n",
      "1   AAPL 2018-02-28  41.819079  45.154999  37.560001  41.909737  3.711577e+09\n",
      "2   AAPL 2018-03-31  43.761786  45.875000  41.235001  43.624048  2.854911e+09\n",
      "3   AAPL 2018-04-30  42.441310  44.735001  40.157501  42.458572  2.664617e+09\n",
      "4   AAPL 2018-05-31  46.239091  47.592499  41.317501  46.384205  2.483905e+09\n"
     ]
    }
   ],
   "source": [
    "# Convert the 'date' column to datetime format for resampling\n",
    "faang_data['date'] = pd.to_datetime(faang_data['date'])\n",
    "\n",
    "# Set 'date' as the DataFrame index\n",
    "faang_data.set_index('date', inplace=True)\n",
    "\n",
    "# Group by ticker and resample to monthly frequency, then apply the required aggregations\n",
    "faang_monthly_agg = faang_data.groupby('ticker').resample('ME').agg({\n",
    "    'open': 'mean',  # Mean of the opening price\n",
    "    'high': 'max',   # Maximum of the high price\n",
    "    'low': 'min',    # Minimum of the low price\n",
    "    'close': 'mean', # Mean of the closing price\n",
    "    'volume': 'sum'  # Sum of the volume traded\n",
    "}).reset_index()\n",
    "\n",
    "# Display the first few rows of the aggregated result\n",
    "print(faang_monthly_agg.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "magType   mb  mb_lg    md   mh   ml  ms_20    mw  mwb  mwr  mww\n",
      "tsunami                                                        \n",
      "0        5.6    3.5  4.11  1.1  4.2    NaN  3.83  5.8  4.8  6.0\n",
      "1        6.1    NaN   NaN  NaN  5.1    5.7  4.41  NaN  NaN  7.5\n"
     ]
    }
   ],
   "source": [
    "# Convert the 'mag' column to numeric in case there are any issues with types\n",
    "earthquake_data['mag'] = pd.to_numeric(earthquake_data['mag'], errors='coerce')\n",
    "\n",
    "# Build a crosstab with the tsunami and magType columns, showing the maximum magnitude for each combination\n",
    "crosstab_result = pd.crosstab(\n",
    "    index=earthquake_data['tsunami'], \n",
    "    columns=earthquake_data['magType'], \n",
    "    values=earthquake_data['mag'], \n",
    "    aggfunc='max'\n",
    ")\n",
    "\n",
    "# Print the crosstab result\n",
    "print(crosstab_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ticker       date        open        high         low       close  \\\n",
      "0      AAPL 2018-01-02         NaN         NaN         NaN         NaN   \n",
      "1      AAPL 2018-01-03         NaN         NaN         NaN         NaN   \n",
      "2      AAPL 2018-01-04         NaN         NaN         NaN         NaN   \n",
      "3      AAPL 2018-01-05         NaN         NaN         NaN         NaN   \n",
      "4      AAPL 2018-01-08         NaN         NaN         NaN         NaN   \n",
      "...     ...        ...         ...         ...         ...         ...   \n",
      "1250   NFLX 2018-12-24  306.018001  386.799988  233.679993  303.239834   \n",
      "1251   NFLX 2018-12-26  303.596001  386.799988  231.229996  301.232167   \n",
      "1252   NFLX 2018-12-27  301.500334  386.799988  231.229996  299.134501   \n",
      "1253   NFLX 2018-12-28  299.393001  380.929993  231.229996  297.116833   \n",
      "1254   NFLX 2018-12-31  297.420168  380.000000  231.229996  295.293667   \n",
      "\n",
      "           volume  \n",
      "0             NaN  \n",
      "1             NaN  \n",
      "2             NaN  \n",
      "3             NaN  \n",
      "4             NaN  \n",
      "...           ...  \n",
      "1250  811001500.0  \n",
      "1251  818289300.0  \n",
      "1252  822147900.0  \n",
      "1253  824502000.0  \n",
      "1254  832212300.0  \n",
      "\n",
      "[1255 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# Define the rolling window size\n",
    "window_size = 60\n",
    "\n",
    "# Group by ticker and calculate rolling aggregations\n",
    "rolling_agg = faang_data.groupby('ticker').rolling(window=window_size).agg({\n",
    "    'open': 'mean',    # Mean of the opening price\n",
    "    'high': 'max',     # Maximum of the high price\n",
    "    'low': 'min',      # Minimum of the low price\n",
    "    'close': 'mean',   # Mean of the closing price\n",
    "    'volume': 'sum'    # Sum of the volume traded\n",
    "}).reset_index()\n",
    "\n",
    "# Display the result\n",
    "print(rolling_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              close         high          low         open        volume\n",
      "ticker                                                                  \n",
      "AAPL      47.263357    47.748526    46.795877    47.277859  1.360803e+08\n",
      "AMZN    1641.726176  1662.839839  1619.840519  1644.072709  5.648994e+06\n",
      "FB       171.510956   173.613347   169.303148   171.472948  2.765860e+07\n",
      "GOOG    1113.225134  1125.777606  1101.001658  1113.554101  1.741965e+06\n",
      "NFLX     319.290319   325.219322   313.187330   319.620558  1.146962e+07\n"
     ]
    }
   ],
   "source": [
    "# Create a pivot table with the average values for OHLC and volume traded by ticker\n",
    "pivot_table = faang_data.pivot_table(\n",
    "    index='ticker',               # Rows = ticker\n",
    "    values=['open', 'high', 'low', 'close', 'volume'],  # Columns = OHLC and volume\n",
    "    aggfunc='mean'                # Aggregation = mean (average)\n",
    ")\n",
    "\n",
    "# Display the pivot table\n",
    "print(pivot_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         open      high       low     close    volume\n",
      "690  2.337813  2.368006  2.502113  2.385848 -1.630411\n",
      "691  2.190795  2.227302  2.247433  2.155037 -0.861879\n",
      "692  2.068570  2.058955  2.139987  2.025489 -0.920345\n",
      "693  1.850048  1.819474  1.781561  1.722816 -0.126582\n",
      "694  1.642819  1.628173  1.554416  1.584748 -0.298771\n",
      "..        ...       ...       ...       ...       ...\n",
      "748 -2.179582 -2.159820 -2.187566 -2.226185 -0.141238\n",
      "749 -2.026617 -1.611714 -1.810493 -1.339674  1.123063\n",
      "750 -1.456521 -1.641276 -1.626703 -1.404343  0.849827\n",
      "751 -1.328549 -1.325261 -1.231588 -1.289951  0.496102\n",
      "752 -1.078283 -1.273456 -0.975763 -1.122691 -0.246405\n",
      "\n",
      "[63 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load the uploaded faang.csv file\n",
    "faang_file_path = 'data/Mod6/faang.csv'\n",
    "faang_data = pd.read_csv(faang_file_path)\n",
    "\n",
    "# Convert the 'date' column to datetime\n",
    "faang_data['date'] = pd.to_datetime(faang_data['date'])\n",
    "\n",
    "# Filter data for Amazon (AMZN) in Q4 2018\n",
    "amzn_q4_2018 = faang_data[(faang_data['ticker'] == 'AMZN') & \n",
    "                          (faang_data['date'] >= '2018-10-01') & \n",
    "                          (faang_data['date'] <= '2018-12-31')]\n",
    "\n",
    "# Calculate Z-scores for each numeric column (open, high, low, close, volume)\n",
    "z_scores = amzn_q4_2018[['open', 'high', 'low', 'close', 'volume']].apply(\n",
    "    lambda x: (x - x.mean()) / x.std()\n",
    ")\n",
    "\n",
    "# Display the Z-scores\n",
    "print(z_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           date ticker         high          low         open        close  \\\n",
      "0    2018-01-02   AAPL    43.075001    42.314999    42.540001    43.064999   \n",
      "1    2018-01-02   AMZN  1190.000000  1170.510010  1172.000000  1189.010010   \n",
      "2    2018-01-02     FB   181.580002   177.550003   177.679993   181.419998   \n",
      "3    2018-01-02   GOOG  1066.939941  1045.229980  1048.339966  1065.000000   \n",
      "4    2018-01-02   NFLX   201.649994   195.419998   196.100006   201.070007   \n",
      "...         ...    ...          ...          ...          ...          ...   \n",
      "1250 2018-12-31   AAPL    39.840000    39.119999    39.632500    39.435001   \n",
      "1251 2018-12-31   AMZN  1520.760010  1487.000000  1510.800049  1501.969971   \n",
      "1252 2018-12-31     FB   134.639999   129.949997   134.449997   131.089996   \n",
      "1253 2018-12-31   GOOG  1052.699951  1023.590027  1050.959961  1035.609985   \n",
      "1254 2018-12-31   NFLX   270.100006   260.000000   260.160004   267.660004   \n",
      "\n",
      "           volume event  \n",
      "0     102223600.0   NaN  \n",
      "1       2694500.0   NaN  \n",
      "2      18151900.0   NaN  \n",
      "3       1237600.0   NaN  \n",
      "4      10966900.0   NaN  \n",
      "...           ...   ...  \n",
      "1250  140014000.0   NaN  \n",
      "1251    6954500.0   NaN  \n",
      "1252   24625300.0   NaN  \n",
      "1253    1493300.0   NaN  \n",
      "1254   13508900.0   NaN  \n",
      "\n",
      "[1255 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create the event DataFrame\n",
    "events_data = pd.DataFrame({\n",
    "    'ticker': ['FB', 'FB', 'FB'],\n",
    "    'date': ['2018-07-25', '2018-03-19', '2018-03-20'],\n",
    "    'event': ['Disappointing user growth announced after close.', \n",
    "              'Cambridge Analytica story', \n",
    "              'FTC investigation']\n",
    "})\n",
    "\n",
    "# Convert 'date' to datetime format\n",
    "events_data['date'] = pd.to_datetime(events_data['date'])\n",
    "\n",
    "# Set the index to ['date', 'ticker']\n",
    "events_data.set_index(['date', 'ticker'], inplace=True)\n",
    "\n",
    "# Set the FAANG data index to ['date', 'ticker'] for merging\n",
    "faang_data.set_index(['date', 'ticker'], inplace=True)\n",
    "\n",
    "# Perform an outer join between FAANG data and events data\n",
    "merged_data = faang_data.merge(events_data, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "# Reset the index to make the merged data easier to view\n",
    "merged_data.reset_index(inplace=True)\n",
    "\n",
    "# Display the merged data\n",
    "print(merged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          open      high       low     close    volume       date ticker\n",
      "251   1.000000  1.000000  1.000000  1.000000  1.000000 2018-01-02   AAPL\n",
      "252   1.013928  1.013059  1.015952  0.999826  1.155033 2018-01-03   AAPL\n",
      "253   1.013987  1.006790  1.016661  1.004470  0.877864 2018-01-04   AAPL\n",
      "254   1.019276  1.017818  1.022392  1.015906  0.925814 2018-01-05   AAPL\n",
      "255   1.024624  1.019211  1.027591  1.012133  0.804816 2018-01-08   AAPL\n",
      "...        ...       ...       ...       ...       ...        ...    ...\n",
      "999   1.234064  1.242995  1.195783  1.163177  0.870583 2018-12-24   NFLX\n",
      "1000  1.192861  1.262088  1.183246  1.261600  1.313288 2018-12-26   NFLX\n",
      "1001  1.275421  1.267493  1.228636  1.271050  1.115648 2018-12-27   NFLX\n",
      "1002  1.315349  1.298835  1.278272  1.273586  1.002362 2018-12-28   NFLX\n",
      "1003  1.326670  1.339450  1.330468  1.331178  1.231788 2018-12-31   NFLX\n",
      "\n",
      "[1255 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load the FAANG data from your local environment\n",
    "faang_file_path = 'data/Mod6/faang.csv'  # Replace with your local file path\n",
    "faang_data = pd.read_csv(faang_file_path)\n",
    "\n",
    "# Convert the 'date' column to datetime format\n",
    "faang_data['date'] = pd.to_datetime(faang_data['date'])\n",
    "\n",
    "# Sort the data by 'ticker' and 'date'\n",
    "faang_data.sort_values(['ticker', 'date'], inplace=True)\n",
    "\n",
    "# Group by 'ticker' and apply the transform method to represent the values in terms of the first date\n",
    "# We'll apply the transformation only to the OHLC and volume columns\n",
    "faang_transformed = faang_data.groupby('ticker')[['open', 'high', 'low', 'close', 'volume']].transform(\n",
    "    lambda x: x / x.iloc[0]\n",
    ")\n",
    "\n",
    "# Add the date and ticker back into the transformed DataFrame for reference\n",
    "faang_transformed['date'] = faang_data['date']\n",
    "faang_transformed['ticker'] = faang_data['ticker']\n",
    "\n",
    "# Display the transformed data\n",
    "print(faang_transformed)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
